<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>e0a7c76952d04ccabfdfd0de99f8d1c7</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="section" class="cell markdown" id="PmkDEdmPNTUS">
<h1><html></h1>
<div style="background-image: linear-gradient(to left, rgb(255, 255, 255), rgb(138, 136, 136)); width: auto; margin: 10px;">
  <img src="https://upload.wikimedia.org/wikipedia/en/thumb/f/fd/University_of_Tehran_logo.svg/225px-University_of_Tehran_logo.svg.png" width=100px width=auto style="padding:10px; vertical-align: center;">

</div>
<div   style:"text-align: center; background-image: linear-gradient(to left, rgb(255, 255, 255), rgb( 219, 204, 245  ));width: 400px; height: 30px; ">
<h1 style="font-family: Georgia; color: black; text-align: center; ">Course: AI </h1>

</div>
<pre><code>&lt;div   style:&quot;border: 3px solid green;text-align: center; &quot;&gt;</code></pre>
<h1 style="font-family: Georgia; color: black; text-align: center; ">Project5: CNN </h1>
</div>
<p><div><br />
<h1 style="font-family: Georgia; color: black; text-align: center; font-size:15px;">Mohammad
Amanlou- sid:810100084 </h1></p>
</div>
</html>
</section>
<section id="artificial-intelligence-course---fall-1402"
class="cell markdown" id="kIiYUdfzNZYZ">
<h1>Artificial Intelligence Course - Fall 1402</h1>
<h2 id="computer-assignment-5---cnn">Computer Assignment #5 - CNN</h2>
</section>
<section id="problem-description" class="cell markdown"
id="wX2Oq6zhUvDz">
<h2>Problem Description</h2>
<p>In this project, we plan to use a CNN neural network to create a
model for classifying brain imaging images. In this model, we check the
existence or non-existence or suspicion of brain tumor in each image and
We put them in one of the related categories</p>
</section>
<section id="part1" class="cell markdown" id="sOBtmi0lNide">
<h1>Part1.</h1>
<h2 id="making-dataset">Making dataset</h2>
</section>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:11.975277Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:11.974800Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:11.988819Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:11.987690Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:11.975236Z&quot;}"
id="AW9oHOyZtntA">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MMDDataset(Dataset):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, directory, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.directory <span class="op">=</span> directory</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.samples <span class="op">=</span> []</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes <span class="op">=</span> []</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_to_idx <span class="op">=</span> {}</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, folder <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">sorted</span>(os.listdir(directory))):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.classes.append(folder)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.class_to_idx[folder] <span class="op">=</span> idx</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            folder_path <span class="op">=</span> os.path.join(directory, folder)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> os.path.isdir(folder_path):</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> filename <span class="kw">in</span> os.listdir(folder_path):</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>                    file_path <span class="op">=</span> os.path.join(folder_path, filename)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> file_path.endswith(<span class="st">&#39;.jpg&#39;</span>):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.samples.append((file_path, idx))</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.samples)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        image_path, class_index <span class="op">=</span> <span class="va">self</span>.samples[idx]</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(image_path).convert(<span class="st">&#39;RGB&#39;</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> <span class="va">self</span>.transform(image)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Debugging print statement:</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(f&quot;__getitem__ returned: {type(image)}, {type(class_index)}&quot;)</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, class_index , <span class="va">self</span>.classes[class_index]</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>to_tensor <span class="op">=</span> transforms.Compose([</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">512</span>, <span class="dv">512</span>)),</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:12.292939Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:12.292020Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:12.307856Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:12.306638Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:12.292876Z&quot;}"
id="k5I6XY43eAh3">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> MMDDataset(directory<span class="op">=</span><span class="st">&#39;/kaggle/input&#39;</span>, transform<span class="op">=</span>to_tensor)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">4</span>)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:386}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:12.640879Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:12.640505Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:22.691881Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:22.690903Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:12.640852Z&quot;}"
id="QPayPMEdqQKf" data-outputId="6b20f21d-e97e-4c1d-ba71-b301537dcd51">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>pics <span class="op">=</span> []</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> []</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_classes):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    found <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image, label, label_name <span class="kw">in</span> dataset:</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> label <span class="op">==</span> i:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            pics.append(image.squeeze(<span class="dv">0</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            labels.append(label_name)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            found <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> found:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;No images found for class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> in the training dataset.&quot;</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> index, tensor <span class="kw">in</span> <span class="bu">enumerate</span>(pics):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    pics[index] <span class="op">=</span> transforms.ToPILImage()(tensor)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">&#39;Sample images from each class&#39;</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (ax, img, lbl) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(axes.flatten(), pics, labels)):</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    ax.imshow(img)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f&#39;Class </span><span class="sc">{</span>lbl<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/64d0908791ca819e0197546f7d36798b713c5a63.png" /></p>
</div>
</div>
<section
id="q1-investigate-the-effect-of-normalization-operation-on-the-number-of-images-and-report-its-advantages-and-disadvantages"
class="cell markdown" id="MtYJFiY_N-I4">
<h3>Q1. Investigate the effect of normalization operation on the number
of images and report its advantages and disadvantages.</h3>
<p>Normalization is a preprocessing step commonly applied to image
datasets before feeding them into a neural network for training or
inference. The process involves changing the range of pixel intensity
values to a standard scale. Here's a general outline of how
normalization can impact datasets and its advantages and
disadvantages.</p>
<h4 id="advantages-of-normalization">Advantages of Normalization:</h4>
<ol>
<li><strong>Improves Convergence:</strong> Normalization helps in
stabilizing the learning process and speeds up convergence by ensuring
that features are on a similar scale.</li>
<li><strong>Reduces Internal Covariate Shift:</strong> By normalizing
the inputs, we reduce the chances of internal covariate shift, which
speeds up the training by allowing for higher learning rates.</li>
<li><strong>Helps to Avoid Saturation:</strong> Neural networks
(especially those with sigmoid activation functions) are at risk of
'saturation,' where extreme input values can 'kill' the gradients during
backpropagation. Normalization helps to keep input values within the
range where the neurons are most sensitive to input changes.</li>
<li><strong>Better Initial Weights:</strong> During weight
initialization, having features in a similar scale allows the optimizer
to better traverse the loss landscape, potentially finding better
minima.</li>
<li><strong>Eliminates Bias:</strong> If certain features come in larger
scales than others, they could dominate the optimization process.
Normalization ensures this does not happen.</li>
</ol>
<h4 id="disadvantages-of-normalization">Disadvantages of
Normalization:</h4>
<ol>
<li><strong>Loss of Some Information:</strong> In cases where the
contrast of the image is important for understanding the content, global
normalization may unnecessarily standardize this aspect away.</li>
<li><strong>Complexity:</strong> It adds additional processing steps in
the data pipeline which sometimes can be complex if different images
require different types of normalization.</li>
<li><strong>Parameter Dependence:</strong> If the normalization step
depends on certain parameters (e.g., mean and standard deviation of the
training set), those need to be saved and correctly applied to data
during inference as well, adding an extra step to the process.</li>
<li><strong>Potential Artifacts:</strong> Some naive or aggressive
normalization approaches might produce artifacts in the images that were
not present prior to processing, which can affect model training.</li>
<li><strong>Requirement of Domain Knowledge:</strong> There might be
cases when domain-specific knowledge is required to decide what type of
normalization is appropriate (e.g., local vs global normalization),
otherwise it can negatively impact the model if chosen incorrectly.</li>
</ol>
<p>Normalization does not change the number of images in the dataset; it
only changes the values of the pixels within those images. The effect on
the dataset is on the form and quality of the data for computational and
learning efficiency, not on the size of the dataset.</p>
<p>However, it's crucial to apply the identical normalization parameters
(for instance, the same mean and standard deviation used for training
images) to the validation and test sets to keep conditions consistent
across the whole data pipeline and avoid data leakage, thus ensuring
that the trained model performs accurately with new data.</p>
</section>
<div class="cell markdown" id="WEbrSLEJOfyr">
<p>Here, after testing both methods with and without normalization, it
was found that without normalization, more accuracy is achieved, because
the missing data after convolution is naturally present. With
normalization, the amount of missing data is It is getting more and
more.</p>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:22.694088Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:22.693775Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:28.690278Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:28.689067Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:22.694061Z&quot;}"
id="q0k5RGNv3bmP">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_mean_std(loader):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    total_images_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, _ , name <span class="kw">in</span> loader:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        batch_samples <span class="op">=</span> images.size(<span class="dv">0</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.view(batch_samples, images.size(<span class="dv">1</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">+=</span> images.mean(<span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        std <span class="op">+=</span> images.<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">2</span>).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        total_images_count <span class="op">+=</span> batch_samples</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">/=</span> total_images_count</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> (std <span class="op">/</span> total_images_count <span class="op">-</span> mean <span class="op">**</span> <span class="dv">2</span>) <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean, std</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>mean, std <span class="op">=</span> compute_mean_std(loader)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>dataset_mean <span class="op">=</span> <span class="bu">list</span>(mean.numpy())</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>dataset_std <span class="op">=</span> <span class="bu">list</span>(std.numpy())</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>transform_with_normalization <span class="op">=</span> transforms.Compose([</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">512</span>, <span class="dv">512</span>)),</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>dataset_mean, std<span class="op">=</span>dataset_std)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>normalized_dataset <span class="op">=</span> MMDDataset(directory<span class="op">=</span><span class="st">&#39;/kaggle/input&#39;</span>, transform<span class="op">=</span>transform_with_normalization)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
</div>
<section
id="q7-explain-about-the-batch-size-parameter-and-its-effect-in-the-training-process"
class="cell markdown" id="AUoaFUFjXQt5">
<h3>Q7. Explain about the batch size parameter and its effect in the
training process</h3>
<p>The batch size is a hyperparameter that defines the number of samples
to work through before updating the internal model parameters. When
training a neural network, it’s not feasible or efficient to update the
model’s weights with every sample (online learning) or after going
through the entire dataset (because of the computational burden and
memory constraints).</p>
<p>So, training often occurs using mini-batches.</p>
<h4 id="effects-of-batch-size-on-training-process">Effects of Batch Size
on Training Process:</h4>
<ol>
<li><strong>Memory Usage</strong>:
<ul>
<li><strong>Smaller batches</strong> use less memory, enabling you to
train the model using limited resources.</li>
<li><strong>Larger batches</strong> consume more memory, but you can
leverage highly optimized matrix multiplication routines.</li>
</ul></li>
<li><strong>Model Performance and Generalization</strong>:
<ul>
<li><strong>Smaller batches</strong> can introduce noise into the
training process, which can lead to stronger generalization; however,
they also make training less stable.</li>
<li><strong>Larger batches</strong> provide a more stable error gradient
for the optimizer leading perhaps to more stable convergence, but they
might also converge to sharp minimizers and thus lead to poorer
generalization.</li>
</ul></li>
<li><strong>Computational Efficiency</strong>:
<ul>
<li><strong>Smaller batches</strong> can result in a more thorough
exploration of the error landscape, while potentially taking longer to
converge to a minimum.</li>
<li><strong>Larger batches</strong> make each iteration take longer, but
the model may require fewer iterations to converge.</li>
</ul></li>
<li><strong>Convergence Speed</strong>:
<ul>
<li><strong>Smaller batches</strong> can sometimes escape local minima
due to the noise in their gradient estimations, potentially leading to
better overall solutions.</li>
<li><strong>Larger batches</strong> tend to have a smoothing effect
which can sometimes cause the optimizer to get stuck in a local
minimum.</li>
</ul></li>
<li><strong>Use of Parallelism</strong>:
<ul>
<li>Larger batches are typically more amenable to parallelization,
allowing for more efficient use of multi-core CPUs and GPUs which can
lead to faster training times overall.</li>
</ul></li>
</ol>
<h4 id="choosing-the-right-batch-size">Choosing the Right Batch
Size:</h4>
<ul>
<li><p>It often requires experimentation as the optimal batch size can
differ depending on the specific dataset and neural network
architecture.</p></li>
<li><p>As a rule of thumb, powers of 2 are often chosen (32, 64, 128,
256, ...) due to the way memory is structured and accessed in modern
computing architectures, allowing for more efficient data
retrieval.</p></li>
</ul>
<h4 id="other-considerations">Other Considerations:</h4>
<ul>
<li><p><strong>Batch size vs. Learning rate</strong>: Large batch sizes
may require a different learning rate than small batch sizes. In some
cases, larger batch sizes have been shown to benefit from learning rate
scaling (linearly scaling up the learning rate with the batch
size).</p></li>
<li><p><strong>Training Stability</strong>: Very large batch sizes may
cause training instability, especially with inadequate learning rate
scaling.</p></li>
<li><p>In practice, many researchers and practitioners will balance the
size of the batch with the available memory of the hardware used for
training, along with empirical experimentation to find the size that
provides a good compromise between training speed and model
performance.</p></li>
</ul>
</section>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:28.691895Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:28.691575Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:28.698892Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:28.697924Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:28.691868Z&quot;}"
id="XdV_mYzU_w_R">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(normalized_dataset))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>test_size <span class="op">=</span> <span class="bu">len</span>(normalized_dataset) <span class="op">-</span> train_size</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> random_split(normalized_dataset, [train_size, test_size])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:911}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:28.702251Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:28.701904Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:29.295252Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:29.294154Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:28.702226Z&quot;}"
id="3SAUqqKBqjap" data-outputId="7fede27a-114d-4d61-8ebe-bc56084f35c1">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_class_distribution(dataset_obj, indices):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    count_dict <span class="op">=</span> {cls: <span class="dv">0</span> <span class="cf">for</span> cls <span class="kw">in</span> dataset_obj.dataset.classes}</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> indices:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        _, class_idx <span class="op">=</span> dataset_obj.dataset.samples[idx]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> dataset_obj.dataset.classes[class_idx]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        count_dict[class_name] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count_dict</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>train_class_counts <span class="op">=</span> get_class_distribution(train_dataset, train_dataset.indices)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>test_class_counts <span class="op">=</span> get_class_distribution(test_dataset, test_dataset.indices)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_class_counts)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_class_counts)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].bar(train_class_counts.keys(), train_class_counts.values())</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">&#39;Class Distribution in Training Set&#39;</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">&#39;Number of Images&#39;</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xticklabels(train_class_counts.keys(), rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].bar(test_class_counts.keys(), test_class_counts.values())</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">&#39;Class Distribution in Test Set&#39;</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">&#39;Number of Images&#39;</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xticklabels(test_class_counts.keys(), rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>{&#39;glioma&#39;: 240, &#39;meningioma&#39;: 237, &#39;notumor&#39;: 327, &#39;pituitary&#39;: 244}
{&#39;glioma&#39;: 60, &#39;meningioma&#39;: 69, &#39;notumor&#39;: 78, &#39;pituitary&#39;: 56}
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/tmp/ipykernel_42/1791657697.py:22: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax[0].set_xticklabels(train_class_counts.keys(), rotation=45)
/tmp/ipykernel_42/1791657697.py:28: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax[1].set_xticklabels(test_class_counts.keys(), rotation=45)
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/e5ce13e9fc327403faa22a72d7cd947b293509c5.png" /></p>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:29.296770Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:29.296470Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:29.302404Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:29.301448Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:29.296743Z&quot;}"
id="P3DTSkTvp-Vs" data-outputId="a7e6dcdb-8525-4eb7-fc06-5ebb4d8338a5">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> dataset.classes</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Clasess Count: &#39;</span>, <span class="bu">len</span>(classes))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Classes: &#39;</span>, classes)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Clasess Count:  4
Classes:  [&#39;glioma&#39;, &#39;meningioma&#39;, &#39;notumor&#39;, &#39;pituitary&#39;]
</code></pre>
</div>
</div>
<section id="part2-image-classification-using-cnn-network"
class="cell markdown" id="y11POWy-OzEA">
<h2>Part2. Image classification using CNN network</h2>
</section>
<section
id="q3-it-is-also-necessary-that-the-necessary-explanations-about-how-they-work-and-also-the-reason-for-using-them-are-given-in-the-report-required-items-in-this-section-include-kernel-size-and-stride-for-different-layers-padding-used-in-different-layers-number-of-convolutional-layers-depth-of-convolutional-network-and"
class="cell markdown" id="J1gzcqAJV_qj">
<h3>Q3. It is also necessary that the necessary explanations about how
they work and also the reason for using them are given in the report.
Required items in this section include kernel size and stride for
different layers, padding used in different layers, number of
convolutional layers, depth of convolutional network and</h3>
<h4 id="convolutional-layers">Convolutional Layers:</h4>
<p>Convolutional layers are the core building blocks of a CNN. They
apply a set of learnable filters to the input image to create feature
maps. These filters detect spatial hierarchies of features—from edges in
the early layers to complex patterns in the deeper layers.</p>
<h4 id="kernel-size">Kernel Size:</h4>
<p>The kernel (or filter) size determines the width and height of the
filter window that slides over the input image to produce a single pixel
in the feature map. For example, a 3x3 filter looks at a 3-pixel by
3-pixel region of the image.</p>
<ul>
<li><strong>Why use different kernel sizes?</strong>
<ul>
<li>Smaller kernels (like 3x3 or 5x5) are good for capturing more
detailed features and are computationally more efficient.</li>
<li>Larger kernels (like 7x7 or beyond) capture broader features of the
image but significantly reduce the spatial dimensions and may miss finer
details.</li>
</ul></li>
</ul>
<h4 id="stride">Stride:</h4>
<p>Stride dictates the number of pixels by which the filter window moves
across the input image. A stride of 1 moves the filter one pixel at a
time, resulting in dense feature mapping. A larger stride reduces the
spatial dimensions of the output feature map, leading to less
computation and memory usage but possibly losing some fine-grained
patterns.</p>
<h4 id="padding">Padding:</h4>
<p>Padding involves adding layers of zeros around the border of the
input image to allow the convolution operation to be applied to the
bordering elements of the input image.</p>
<ul>
<li><strong>Why use padding?</strong>
<ul>
<li>To control the spatial size of the output volumes, often to maintain
the same input and output dimensions.</li>
<li>To enable the network to learn features from the corners and edges
of the images.</li>
</ul></li>
</ul>
<h4 id="number-of-convolutional-layers">Number of Convolutional
Layers:</h4>
<p>This refers to the depth of the network, the number of convolutional
layers stacked together. Increasing depth can enhance the network's
ability to represent complex features.</p>
<ul>
<li><strong>Why adjust the number of layers?</strong>
<ul>
<li>More layers can help the network learn a hierarchical representation
of the data.</li>
<li>Too many layers might lead to overfitting and make the network
harder to train, especially without sufficient data.</li>
<li>As you go deeper, the spatial size tends to decrease whereas the
depth (number of filters) increases, moving from raw pixels to abstract
concepts.</li>
</ul></li>
</ul>
<h4 id="depth-of-convolutional-network">Depth of Convolutional
Network:</h4>
<p>The term "depth" can refer to two things: the number of layers or the
number of channels (feature maps) in each layer.</p>
<ul>
<li><strong>Why increase depth?</strong>
<ul>
<li>Deeper networks can potentially learn more complex features and
achieve better performance.</li>
<li>However, they can be challenging to train due to issues like
vanishing gradients, although techniques such as skip connections (e.g.,
ResNet) and batch normalization help overcome these challenges.</li>
<li>Deeper channels (more filters) in a layer allow a network to learn a
wider variety of features from the same level of complexity in the
data.</li>
</ul></li>
</ul>
<h4 id="considerations">Considerations:</h4>
<p>The choice of kernel size, stride, and padding also interacts with
the image size and the number of convolutional layers to determine the
overall architecture of the network. Properly sizing and structuring the
network is as much an art as a science and often involves empirical
testing and tuning.</p>
<p>When designing a CNN architecture, you often start with industry
standards or proven network architectures (such as VGG, ResNet,
Inception, etc.) and then customize and adapt based on your specific
problem and dataset size. The goal is to balance performance (in terms
of both prediction quality and computational efficiency) with the
avoidance of overfitting, under the constraints of what's meaningful for
your data and the problem at hand. Hyperparameter tuning and network
architecture searching are extensive fields that seek to automate this
part of model development.</p>
</section>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:29.304641Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:29.303967Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:33.699265Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:33.698300Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:29.304604Z&quot;}"
id="N609rgh8kWsn">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelBrain(nn.Module):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ModelBrain, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the convolution part with added layers</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_layer <span class="op">=</span> nn.Sequential(</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(num_channels, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._to_linear <span class="op">=</span> <span class="dv">16</span>  <span class="op">*</span><span class="dv">124</span><span class="op">*</span><span class="dv">124</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_layer <span class="op">=</span> nn.Sequential(</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(),</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>._to_linear, hidden_units),</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_units, hidden_units <span class="op">//</span> <span class="dv">4</span>),</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_units <span class="op">//</span> <span class="dv">4</span>, output_shape)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv_layer(x)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>._to_linear)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc_layer(x)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>num_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>hidden_units <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>output_shape <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ModelBrain(num_channels<span class="op">=</span>num_channels, hidden_units<span class="op">=</span>hidden_units, output_shape<span class="op">=</span>output_shape)</span></code></pre></div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:26:15.878793Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:26:15.877937Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:26:15.924635Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:26:15.923554Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:26:15.878756Z&quot;}"
id="5VwadV7kUvD9">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Inception(nn.Module):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Inception, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.branch1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels, ch1x1, kernel_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(ch1x1),</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.branch2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels, ch3x3red, kernel_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(ch3x3red),</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(ch3x3red, ch3x3, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(ch3x3),</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.branch3 <span class="op">=</span> nn.Sequential(</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels, ch5x5red, kernel_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(ch5x5red),</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(ch5x5red, ch5x5, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(ch5x5),</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.branch4 <span class="op">=</span> nn.Sequential(</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels, pool_proj, kernel_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(pool_proj),</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(<span class="va">True</span>),</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> [<span class="va">self</span>.branch1(x), <span class="va">self</span>.branch2(x), <span class="va">self</span>.branch3(x), <span class="va">self</span>.branch4(x)]</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat(outputs, <span class="dv">1</span>)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GoogLeNet(nn.Module):</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GoogLeNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.maxpool1 <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">192</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.maxpool2 <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># GoogLeNet has 9 inception modules in the middle, with various configurations</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For simplicity, I am including only a few of them</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inception3a <span class="op">=</span> Inception(<span class="dv">192</span>, <span class="dv">64</span>, <span class="dv">96</span>, <span class="dv">128</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">32</span>)</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inception3b <span class="op">=</span> Inception(<span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">128</span>, <span class="dv">192</span>, <span class="dv">32</span>, <span class="dv">96</span>, <span class="dv">64</span>)</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.maxpool3 <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inception4a <span class="op">=</span> Inception(<span class="dv">480</span>, <span class="dv">192</span>, <span class="dv">96</span>, <span class="dv">208</span>, <span class="dv">16</span>, <span class="dv">48</span>, <span class="dv">64</span>)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: Auxiliary classifiers are typically inserted here, but are omitted for brevity</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.maxpool4 <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The number of inception modules and their arrangements can be modified</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># depending on the input size and the complexity of the dataset</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ...</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avgpool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.1</span>)</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">512</span>, <span class="dv">128</span>)</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, num_classes)</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add print statements to debug the size of output at each stage</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After conv1:&quot;, x.shape)</span></span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.maxpool1(x)</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After maxpool1:&quot;, x.shape)</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After conv2:&quot;, x.shape)</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After conv3:&quot;, x.shape)</span></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.maxpool2(x)</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After maxpool2:&quot;, x.shape)</span></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.inception3a(x)</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After inception3a:&quot;, x.shape)</span></span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.inception3b(x)</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After inception3b:&quot;, x.shape)</span></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.maxpool3(x)</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After maxpool3:&quot;, x.shape)</span></span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.inception4a(x)</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># More inception modules can be inserted here</span></span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After last inception module:&quot;, x.shape)</span></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.maxpool4(x)</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After avgpool:&quot;, x.shape)</span></span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(&quot;After flatten:&quot;, x.shape)</span></span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>Gmodel <span class="op">=</span> GoogLeNet(num_classes<span class="op">=</span><span class="dv">4</span>)</span></code></pre></div>
</div>
<div class="cell markdown" id="U-4YoA8cT6eY">
<h4 id="nnconv2d-parameters">nn.Conv2d Parameters:</h4>
<ol>
<li><code>num_channels</code>: Number of input channels for the first
convolutional layer. For RGB images, this is 3.</li>
<li><code>out_channels</code>: Number of filters or kernels the
convolutional layer will learn from the input. Here, it's set to 16,
meaning there will be 16 feature maps after this layer.</li>
<li><code>kernel_size</code>: Size of the convolution filter. A
<code>kernel_size</code> of 10 means a 10x10 filter.</li>
<li><code>padding</code>: Zero-padding added to the borders of the input
matrix. Padding of 1 adds a single layer of 0s around the image, which
enables the kernel to process the edges of the input image.</li>
</ol>
<h4 id="nnrelu">nn.ReLU:</h4>
<p>It's an activation function that introduces non-linearity into the
model, allowing it to learn more complex patterns. The ReLU function is
defined as <code>f(x) = max(0, x)</code>.</p>
<h4 id="nnavgpool2d">nn.AvgPool2d:</h4>
<ol>
<li><code>kernel_size</code>: The size of the window over which the
average value is taken. Here, it's 2x2, which means it reduces the
height and width of the feature maps by a factor of 2.</li>
<li><code>stride</code>: Controls how much the pooling window moves for
each pooling step. In this case, it also is 2, meaning no overlap
between pooled regions.</li>
</ol>
<h4 id="fully-connected-layers---nnlinear">Fully connected layers -
nn.Linear:</h4>
<ol>
<li><code>self._to_linear</code>: This is not a standard PyTorch
parameter, rather it's a user-defined value computed based on the output
shape of the <code>conv_layer</code>. It's critical to update this
calculation because the convolution and pooling operations will alter
the size of the output feature map. For a convolution with padding 1 and
kernel size 10, you actually need to re-calculate the output
dimensions.</li>
<li><code>hidden_units</code>: The number of neurons in the hidden layer
of the fully connected part. It's set to 256 and is a hyperparameter
that can be tuned.</li>
<li><code>output_shape</code>: The size of the output layer, which
corresponds to the number of classes in a classification task. Here,
it's set to 4.</li>
</ol>
<h4 id="forward-pass">Forward pass:</h4>
<p>The <code>forward</code> method defines how your data moves through
the network. The input <code>x</code> goes through the convolutional
layers, then it's flattened to match the dimensionality expected by the
fully connected layers, and finally, it is passed through these linear
layers to produce the output.</p>
<h4 id="effects-of-changing-parameters-and-hyperparameters">Effects of
changing parameters and hyperparameters:</h4>
<ol>
<li>Increasing <code>num_channels</code>: This is typically based on the
input data (e.g., RGB images have 3 channels, grayscale has 1). Changing
it arbitrarily won't make sense.</li>
<li>More <code>out_channels</code>: More filters capture additional
features but increase computational cost and model complexity.</li>
<li>Larger <code>kernel_size</code>: Larger kernels capture more context
but reduce feature map size more drastically.</li>
<li>Increasing <code>padding</code>: Helpful to preserve feature map
size, can lead to better edge feature extraction; too much padding can
dilute feature intensity.</li>
<li>More <code>hidden_units</code>: A higher number allows the network
to learn more complex patterns but may lead to overfitting and increased
computational costs.</li>
<li>Changing <code>output_shape</code>: It's based on the number of
output classes you have; changing it otherwise will mismatch the
problem's requirements.</li>
</ol>
<p>Additionally:</p>
<ul>
<li><p><strong>Number of Layers</strong>: Adding more layers can help
the network learn deeper, more abstract patterns. However, it can also
lead to overfitting and increased computational demands. It's a delicate
balance and must be informed by cross-validation performance.</p></li>
<li><p><strong>Batch Size</strong>: Larger batch sizes result in more
stable gradient estimates during training but can cause memory issues.
Smaller batch sizes offer more updates per epoch but can be
noisy.</p></li>
<li><p><strong>Learning Rate</strong>: Controls the size of the update
steps during optimization. A high learning rate might converge quickly
but can overshoot minima, while a low learning rate might get stuck in
local minima or take a long time to converge.</p></li>
</ul>
<p>Remember that adjusting these parameters can significantly impact how
well your model learns from the training data, how quickly it trains,
and how well it generalizes to unseen data. It's often necessary to
experiment and use techniques like cross-validation to determine optimal
values for your specific task and dataset.</p>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:33.700796Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:33.700482Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:34.510629Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:34.509698Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:33.700768Z&quot;}"
id="w_y8qXqKukRD">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span></code></pre></div>
</div>
<section
id="q4-it-is-necessary-to-mention-the-different-cost-functions-how-they-work-the-application-of-each-of-them-and-also-the-reason-for-choosing-your-final-cost-function"
class="cell markdown" id="aU1uZ4k3Wi70">
<h3>Q4. It is necessary to mention the different cost functions, how
they work, the application of each of them and also the reason for
choosing your final cost function.</h3>
<p>Cost functions, also known as loss functions, are a crucial component
in training neural networks. They measure the difference between the
network's predicted output and the actual target values. This difference
or "loss" is used to adjust the model parameters during training with
the goal of minimizing the loss. Below are several common cost
functions, how they work, their applications, and the rationale behind
selecting a particular cost function.</p>
<h4 id="mean-squared-error-mse-loss">Mean Squared Error (MSE) Loss:</h4>
<ul>
<li><strong>How it Works</strong>: It calculates the average squared
difference between the predicted values and the actual values.</li>
<li><strong>Applications</strong>: MSE is widely used for regression
problems where you need to predict continuous outputs.</li>
<li><strong>Why Choose It</strong>: It penalizes larger errors more than
smaller ones due to squaring the error terms, which can be desirable in
some regression problems.</li>
</ul>
<h4 id="cross-entropy-loss-also-known-as-log-loss">Cross-Entropy Loss
(also known as Log Loss):</h4>
<ul>
<li><strong>How it Works</strong>: It measures the performance of a
classification model whose output is a probability between 0 and 1.
Cross-entropy loss increases as the predicted probability diverges from
the actual label.</li>
<li><strong>Applications</strong>: It’s commonly used in classification
tasks, including multi-class classification and binary
classification.</li>
<li><strong>Why Choose It</strong>: It’s well-suited for probability
distributions and provides a high penalty for predictions that are
confidently incorrect, which drives the model towards confident and
correct predictions.</li>
</ul>
<h4 id="binary-cross-entropy-loss">Binary Cross-Entropy Loss:</h4>
<ul>
<li><strong>How it Works</strong>: A special case of cross-entropy loss
for binary classification problems. It calculates the loss for each
label in the dataset.</li>
<li><strong>Applications</strong>: Used when there are only two label
classes (e.g. spam vs not-spam).</li>
<li><strong>Why Choose It</strong>: It’s effective in scenarios where
you need the probabilities of binary outcomes.</li>
</ul>
<h4 id="negative-log-likelihood-loss">Negative Log-Likelihood Loss:</h4>
<ul>
<li><strong>How it Works</strong>: This loss is used when the model
outputs a log-probability. It is minimized when the model's predicted
probability of the correct class is maximized.</li>
<li><strong>Applications</strong>: Commonly used with models ending with
a softmax activation function for classification tasks.</li>
<li><strong>Why Choose It</strong>: Functions well with probabilities
when you have a multinomial distribution and can provide more stable
convergence than direct probability outputs.</li>
</ul>
<h4 id="hinge-loss">Hinge Loss:</h4>
<ul>
<li><strong>How it Works</strong>: Designed for binary classification
tasks. It is used for "maximum-margin" classification, notably for
support vector machines (SVMs).</li>
<li><strong>Applications</strong>: Often used in training classifiers
like SVMs, and sometimes in neural networks for binary
classification.</li>
<li><strong>Why Choose It</strong>: It encourages the model to classify
correctly and with confidence by penalizing predictions that are not
only wrong but also those that are not sufficiently far from the
classification boundary.</li>
</ul>
<h4 id="categorical-cross-entropy-loss">Categorical Cross-Entropy
Loss:</h4>
<ul>
<li><strong>How it Works</strong>: It is the extension of binary
cross-entropy loss to multi-class classification tasks.</li>
<li><strong>Applications</strong>: Used when there are two or more label
classes. Each label is one-hot encoded.</li>
<li><strong>Why Choose It</strong>: Effective for multi-class
classification problems, especially when each example belongs to a
single category.</li>
</ul>
<h4 id="kullback-leibler-kl-divergence-loss">Kullback-Leibler (KL)
Divergence Loss:</h4>
<ul>
<li><strong>How it Works</strong>: Measures how one probability
distribution diverges from a second, reference probability
distribution.</li>
<li><strong>Applications</strong>: Commonly used in variational
autoencoders (VAEs) and in situations where you want to match to a
particular distribution.</li>
<li><strong>Why Choose It</strong>: Useful for problems where you have a
known target distribution you want to match.</li>
</ul>
<h4 id="choosing-a-cost-function">Choosing a Cost Function:</h4>
<p>The choice of a cost function depends on the specific characteristics
of the problem:</p>
<ul>
<li>In <strong>regression tasks</strong>, you would typically use MSE or
Mean Absolute Error (MAE) if you're concerned about outlier
robustness.</li>
<li>For <strong>classification</strong>, cross-entropy loss functions
are standard because they work well with probability distributions,
which is what classification models output.</li>
<li>If you are working with models that need to be confident and
accurate at the same time (like in certain classification tasks or
SVMs), hinge loss might be appropriate.</li>
<li>Specialized loss functions like KL divergence are used for specific
objectives, such as when trying to approximate complex probability
distributions.</li>
</ul>
<p>When constructing and training a CNN, the final cost function should
align with your model's architecture and the specific type of problem
you're trying to solve. It should effectively drive the training process
toward an accurately predictive model while also maintaining
computational stability and efficiency.</p>
</section>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:34.512097Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:34.511687Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:34.517401Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:34.516344Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:34.512068Z&quot;}"
id="yoERBk92uo3D">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span></code></pre></div>
</div>
<section
id="q5-learn-about-the-adam-optimizer-and-briefly-describe-how-it-works-and-how-it-differs-from-sdg-also-if-you-use-any-optimization-for-the-model-briefly-describe-your-reason"
class="cell markdown" id="1XLQHbnSWzvT">
<h3>Q5. Learn about the Adam optimizer and briefly describe how it works
and how it differs from SDG. Also, if you use any optimization for the
model, briefly describe your reason</h3>
<p>The Adam (Adaptive Moment Estimation) optimizer is a popular
algorithm for training neural networks. It combines ideas from two other
optimizations algorithms: RMSprop (Root Mean Square Propagation) and SGD
(Stochastic Gradient Descent) with momentum.</p>
<h4 id="how-adam-works">How Adam Works:</h4>
<p>Adam maintains two running averages for each weight in the neural
network:</p>
<ol>
<li><strong>The first moment</strong> – This is essentially the running
average of the gradients, similar to the momentum in SGD, which helps to
accelerate the training in the right direction.</li>
<li><strong>The second moment</strong> – This is the running average of
the gradient squared, similar to RMSprop, and it adapts the learning
rate for each weight based on the magnitudes of its gradients.</li>
</ol>
<p>The running averages are initialized as vectors of 0's at the start.
They're updated at each iteration during training. The moment estimates
are then bias-corrected to counteract their initialization at the
origin. The corrected moments are used to update the weights, combining
the benefits of momentum and adaptive learning rates.</p>
<h4 id="adam-vs-sgd">Adam vs. SGD:</h4>
<p>While the standard SGD updates weights by taking a step proportional
only to the average of the recent gradients (optionally, with a momentum
term), Adam modifies this process by scaling the step size for each
weight based on the historical squared gradients (the variance).</p>
<p>Main differences include:</p>
<ol>
<li><strong>Adaptive Learning Rates</strong>: SGD maintains a single
learning rate (step size) for all weight updates which doesn't change.
Adam computes individual adaptive learning rates for different
parameters from estimates of first and second moments of the
gradients.</li>
<li><strong>Momentum</strong>: While momentum can be used alongside SGD,
Adam has it incorporated by design. In Adam, the accumulation of past
gradients furnishes a momentum effect, smoothing the optimization
process.</li>
<li><strong>Bias Correction</strong>: Because Adam calculates adaptive
learning rates based on running averages that start out as zero, it
includes a bias correction mechanism to compensate for this early
underestimation.</li>
</ol>
<h4 id="choosing-adam-for-optimization">Choosing Adam for
Optimization:</h4>
<p>Adam is often chosen as the default optimizer for many deep learning
tasks because:</p>
<ul>
<li>It requires little tuning of hyperparameters (often the default
values of alpha=0.001, beta1=0.9, beta2=0.999, and epsilon=10^-7 work
well).</li>
<li>It combines the advantages of other optimization techniques, making
it suitable for a wide range of problems and data modalities.</li>
<li>It's less sensitive to the scale of the gradients, which is a
benefit in models and problems where the gradient magnitudes vary
significantly.</li>
</ul>
<p>Adam is especially popular in scenarios where you need quick and
efficient convergence, especially when working with large datasets
and/or high parameter models such as deep learning architectures.
However, there are cases where SGD might converge to a better (or more
stable) solution given enough time and careful tuning of the learning
rate schedule. Some recent works have suggested that sometimes models
trained with SGD generalize better than those trained with Adam, but
this is not a one-size-fits-all situation.</p>
<p>Ultimately, the choice of optimizer is another hyperparameter to
tune, and considering computational efficiency, ease of use, and
empirical performance on the validation set should inform the
decision.</p>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:35}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:34.519148Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:34.518707Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:44:34.531532Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:44:34.530439Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:34.519083Z&quot;}"
id="czgEwegEyBV3" data-outputId="688129b8-810b-4e8f-8f4e-3e1613957724">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div>
<div class="output execute_result" data-execution_count="16">
<pre><code>&#39;cuda&#39;</code></pre>
</div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000,&quot;referenced_widgets&quot;:[&quot;10f04ea7bb9d4587a0271fadf006ba74&quot;,&quot;145d351ae15d40588341e9afaefaeb43&quot;,&quot;f338d4df3bc24368ae8f86bdd7d6c2aa&quot;,&quot;acdf8aa651f64a79ba30b95ec8db3e52&quot;,&quot;131fbd7607364edb9f0e67b1d57947cc&quot;,&quot;035e78826bc5449f8a3633b27cdd1722&quot;,&quot;ba9818b413f94923bd5253599438ed55&quot;,&quot;3a093b70f09e41af9801ca0883656b5e&quot;,&quot;f7ede4aa8b7d4f8dad26231109fe42be&quot;,&quot;d142d41784d147309a9e316349d01240&quot;,&quot;007f5bc89d5f41048183e622be486ca1&quot;,&quot;a67c28a48c7c498da8c23fb4599fb56e&quot;]}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:44:40.832534Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:44:40.831690Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:55:38.331819Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:55:38.330630Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:44:40.832499Z&quot;}"
id="DP872uKnu0wp" data-outputId="195834f0-e8ee-4ada-8083-5b622b76460f">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Lists to keep track of progress</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>epoch_count <span class="op">=</span> []</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>train_losses <span class="op">=</span> []</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>test_losses <span class="op">=</span> []</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>train_accuracies <span class="op">=</span> []</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>test_accuracies <span class="op">=</span> []</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    correct_train <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    correct_test <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    total_train <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    total_test <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Epoch </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(epoch<span class="op">+</span><span class="dv">1</span>, epochs))</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;-&quot;</span><span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    model.train()  <span class="co"># Set model to training mode</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels, _ <span class="kw">in</span> train_loader:</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()  <span class="co"># Clear gradients</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(outputs, labels)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>        total_train <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>        correct_train <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set model to evaluation mode</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels, _ <span class="kw">in</span> test_loader:</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> images.to(device)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.to(device)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(outputs, labels)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>            total_test <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>            correct_test <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train_loss <span class="op">/</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> test_loss <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> correct_train <span class="op">/</span> total_train <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>    test_accuracy <span class="op">=</span> correct_test <span class="op">/</span> total_test <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Train Loss: </span><span class="sc">{:.4f}</span><span class="st">, Train Accuracy: </span><span class="sc">{:.2f}</span><span class="st">%&quot;</span>.<span class="bu">format</span>(train_loss, train_accuracy))</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Test Loss: </span><span class="sc">{:.4f}</span><span class="st">, Test Accuracy: </span><span class="sc">{:.2f}</span><span class="st">%&quot;</span>.<span class="bu">format</span>(test_loss, test_accuracy))</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>    epoch_count.append(epoch<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>    train_losses.append(train_loss)</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss)</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>    train_accuracies.append(train_accuracy)</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>    test_accuracies.append(test_accuracy)</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, train_losses, label<span class="op">=</span><span class="st">&#39;Train Loss&#39;</span>)</span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, test_losses, label<span class="op">=</span><span class="st">&#39;Test Loss&#39;</span>)</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Loss over Epochs&#39;</span>)</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, train_accuracies, label<span class="op">=</span><span class="st">&#39;Train Accuracy&#39;</span>)</span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, test_accuracies, label<span class="op">=</span><span class="st">&#39;Test Accuracy&#39;</span>)</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Accuracy (%)&#39;</span>)</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Accuracy over Epochs&#39;</span>)</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb19"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;a67c28a48c7c498da8c23fb4599fb56e&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>Epoch 1/9
----------
Train Loss: 1.4369, Train Accuracy: 31.30%
Test Loss: 1.2452, Test Accuracy: 29.66%
Epoch 2/9
----------
Train Loss: 1.0750, Train Accuracy: 57.63%
Test Loss: 0.8404, Test Accuracy: 65.40%
Epoch 3/9
----------
Train Loss: 0.6379, Train Accuracy: 75.29%
Test Loss: 0.6056, Test Accuracy: 73.76%
Epoch 4/9
----------
Train Loss: 0.5815, Train Accuracy: 79.48%
Test Loss: 0.5880, Test Accuracy: 79.47%
Epoch 5/9
----------
Train Loss: 0.3546, Train Accuracy: 87.79%
Test Loss: 0.5278, Test Accuracy: 80.99%
Epoch 6/9
----------
Train Loss: 0.5919, Train Accuracy: 88.74%
Test Loss: 1.0362, Test Accuracy: 63.50%
Epoch 7/9
----------
Train Loss: 0.3657, Train Accuracy: 87.69%
Test Loss: 0.6166, Test Accuracy: 82.89%
Epoch 8/9
----------
Train Loss: 0.2226, Train Accuracy: 93.61%
Test Loss: 0.7942, Test Accuracy: 81.75%
Epoch 9/9
----------
Train Loss: 0.1143, Train Accuracy: 96.18%
Test Loss: 0.8893, Test Accuracy: 85.55%
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/069ca0f83d0ee2c34acae203128ce35f951246e8.png" /></p>
</div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:56:00.717422Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:56:00.716856Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:56:00.725620Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:56:00.724331Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:56:00.717381Z&quot;}"
id="W-IC6PtV02SD">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_confusion_matrix(real_labels , pred_labels):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    classes <span class="op">=</span> train_dataset.dataset.classes</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(real_labels, pred_labels)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a heatmap</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    sns.<span class="bu">set</span>(font_scale<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    sns.heatmap(cm, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">&quot;d&quot;</span>, cmap <span class="op">=</span> <span class="st">&quot;RdYlGn&quot;</span>, cbar<span class="op">=</span><span class="va">False</span>,xticklabels<span class="op">=</span>classes, yticklabels<span class="op">=</span>classes)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Predicted Labels&quot;</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;True Labels&quot;</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Confusion Matrix&quot;</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cm</span></code></pre></div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:56:02.688358Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:56:02.688014Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:56:02.699560Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:56:02.698371Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:56:02.688333Z&quot;}"
id="dkMOpcKevNWs">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> mpatches</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model, data_loader):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>() <span class="co"># Set model to eval mode</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    true_preds, num_preds <span class="op">=</span> <span class="fl">0.</span>, <span class="fl">0.</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    predicts <span class="op">=</span> []</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    reality <span class="op">=</span> []</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad(): <span class="co"># Deactivate gradients for the following code</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data_inputs, data_labels , name <span class="kw">in</span> data_loader:</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Determine prediction of model on dev set</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>            data_inputs<span class="op">=</span> data_inputs.to(device)<span class="co">#, data_labels.to(device) , name.to(device)</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> model(data_inputs)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> preds.squeeze(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> torch.sigmoid(preds) <span class="co"># Sigmoid to map predictions between 0 and 1</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>            pred_labels <span class="op">=</span> ([torch.argmax(i) <span class="cf">for</span> i <span class="kw">in</span> preds]) <span class="co"># Binarize predictions to 0 and 1</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>            pred_labels <span class="op">=</span> torch.tensor([i.item() <span class="cf">for</span> i <span class="kw">in</span> pred_labels])</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>            true_preds <span class="op">+=</span> (pred_labels <span class="op">==</span> data_labels).<span class="bu">sum</span>()</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>            num_preds <span class="op">+=</span> data_labels.shape[<span class="dv">0</span>]</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p , r <span class="kw">in</span> <span class="bu">zip</span>(pred_labels,data_labels):</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>                predicts.append(p)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>                reality.append(r)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> make_confusion_matrix(reality , predicts)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> true_preds <span class="op">/</span> num_preds</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels , name <span class="kw">in</span> test_loader:</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> images.to(device)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.to(device)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(outputs, labels)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> test_loss <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Test Loss is:</span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>( test_loss))</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Accuracy of the model: </span><span class="sc">{</span><span class="fl">100.0</span><span class="op">*</span>acc<span class="sc">:4.2f}</span><span class="ss">%&quot;</span>)</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cm</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:690}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:56:04.585632Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:56:04.585204Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:56:12.150349Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:56:12.149206Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:56:04.585598Z&quot;}"
id="QIJ0XyLP0ZGi" data-outputId="d5b06a3f-e381-4c50-e620-1891c8d40080">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> eval_model(model ,test_loader )</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/e8f6eae2a31017ef5eff145d6ecb118132bd88c6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Test Loss is:0.8893
Accuracy of the model: 85.17%
</code></pre>
</div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:59:53.903591Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:59:53.903197Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:59:53.919870Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:59:53.918932Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:59:53.903561Z&quot;}"
id="as3w7pJSMHXo">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scoring(cm):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    classes <span class="op">=</span> train_dataset.dataset.classes</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    col_sum <span class="op">=</span> [cm[<span class="dv">0</span>][i] <span class="op">+</span> cm[<span class="dv">1</span>][i]  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> [cm[i][i]<span class="op">/</span><span class="bu">sum</span>(cm[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">4</span>)]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    recalls <span class="op">=</span> [cm[i][i] <span class="op">/</span> col_sum[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    F1_score <span class="op">=</span> [(<span class="dv">2</span><span class="op">*</span>precision[i]<span class="op">*</span>recalls[i])<span class="op">/</span> (precision[i] <span class="op">+</span> recalls[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)]</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> <span class="bu">sum</span>(cm[i][i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>))<span class="op">/</span><span class="bu">sum</span>(<span class="bu">sum</span>(cm))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    plt.clf()</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    plt.scatter(classes, precision)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    plt.scatter(classes, recalls)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    plt.scatter(classes, F1_score)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;class&quot;</span>)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;metric&quot;</span>)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    blue_patch <span class="op">=</span> mpatches.Patch(color<span class="op">=</span><span class="st">&#39;blue&#39;</span>,label<span class="op">=</span><span class="st">&#39;precision&#39;</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    orange_patch <span class="op">=</span> mpatches.Patch(color<span class="op">=</span><span class="st">&#39;orange&#39;</span>,label<span class="op">=</span><span class="st">&#39;F1_score&#39;</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    green_patch <span class="op">=</span> mpatches.Patch(color<span class="op">=</span><span class="st">&#39;green&#39;</span>,label<span class="op">=</span><span class="st">&#39;recalls&#39;</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    plt.legend(handles<span class="op">=</span>[blue_patch,orange_patch,green_patch])</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding the exact values on the plot</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i , c <span class="kw">in</span> <span class="bu">enumerate</span>(classes):</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        plt.text(c, precision[i], <span class="ss">f&#39;</span><span class="sc">{</span>precision[i]<span class="sc">:.2f}</span><span class="ss">&#39;</span>, ha<span class="op">=</span><span class="st">&#39;center&#39;</span>, va<span class="op">=</span><span class="st">&#39;bottom&#39;</span>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        plt.text(c, recalls[i], <span class="ss">f&#39;</span><span class="sc">{</span>recalls[i]<span class="sc">:.2f}</span><span class="ss">&#39;</span>, ha<span class="op">=</span><span class="st">&#39;center&#39;</span>, va<span class="op">=</span><span class="st">&#39;bottom&#39;</span>)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>        plt.text(c, F1_score[i], <span class="ss">f&#39;</span><span class="sc">{</span>F1_score[i]<span class="sc">:.2f}</span><span class="ss">&#39;</span>, ha<span class="op">=</span><span class="st">&#39;center&#39;</span>, va<span class="op">=</span><span class="st">&#39;bottom&#39;</span>)</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    plt.xticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Precision: </span><span class="sc">{</span>precision[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Recall: </span><span class="sc">{</span>recalls[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;F1 Score: </span><span class="sc">{</span>F1_score[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    weighted_average <span class="op">=</span> <span class="bu">sum</span>(F1_score)<span class="op">/</span><span class="dv">4</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    micro_average <span class="op">=</span> (<span class="bu">sum</span>(cm[i][i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)) <span class="op">/</span> <span class="bu">sum</span>(<span class="bu">sum</span>(cm))) <span class="op">+</span> (<span class="bu">sum</span>(cm[i][i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)) <span class="op">/</span> <span class="bu">sum</span>(<span class="bu">sum</span>(cm))) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    macro_average <span class="op">=</span> <span class="bu">sum</span>(F1_score)<span class="op">/</span><span class="dv">4</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;micro average: &quot;</span> <span class="op">+</span> <span class="bu">str</span>(micro_average))</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;weighted_average: &quot;</span> <span class="op">+</span> <span class="bu">str</span>(weighted_average))</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;macro average: &quot;</span> <span class="op">+</span> <span class="bu">str</span>(macro_average))</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;accuract: &quot;</span> <span class="op">+</span> <span class="bu">str</span>(accuracy))</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:620}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T06:59:55.276628Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T06:59:55.275959Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T06:59:55.608620Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T06:59:55.607403Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T06:59:55.276592Z&quot;}"
id="ojMfBNjX96w-" data-outputId="7090cb0c-107a-4bf9-b018-20639cbf4088">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>scoring(cm)</span></code></pre></div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/593e6df46d70a5458dfdd8be01402fae630ee692.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Precision: 0.92
Recall: 0.93
F1 Score: 0.92

micro average: 1.064638783269962
weighted_average: 1.21447943931671
macro average: 1.21447943931671
accuract: 0.8517110266159695
</code></pre>
</div>
</div>
<section id="part3-regularization" class="cell markdown"
id="qi8lJpei8MMF">
<h2>Part3. Regularization.</h2>
</section>
<section
id="q8-two-very-common-methods-to-do-this-are-using-dropout-and-batch-normalization-first-investigate-and-report-on-the-performance-of-each-of-these-two-methods"
class="cell markdown" id="4KTi7zAA8SBW">
<h3>Q8. Two very common methods to do this are using Dropout and Batch
Normalization. First, investigate and report on the performance of each
of these two methods.</h3>
<p>Dropout and Batch Normalization are important techniques in training
deep neural networks. They address different issues and can be applied
independently or together. Below is an overview of each method's purpose
and performance impacts when used in neural networks.</p>
<h4 id="dropout">Dropout</h4>
<p>Dropout is a regularization technique that prevents overfitting by
randomly dropping units (along with their connections) from the neural
network during training. This means that each time an input is passed
through the network, a subset of neurons does not contribute to the
forward pass and the subsequent backpropagation. As a result, the model
becomes less sensitive to the specific weights of neurons and more
robust to unseen data.</p>
<p><strong>Performance Impact</strong>:</p>
<ul>
<li><strong>Overfitting Reduction</strong>: Networks tend to generalize
better and thus perform better on unseen data.</li>
<li><strong>Network Robustness</strong>: It encourages the network to
develop redundant pathways, increasing robustness.</li>
<li><strong>Training Time</strong>: Can increase training time since it
often requires more epochs for convergence.</li>
<li><strong>Inference Time</strong>: No impact since dropout is only
applied during training, not during inference.</li>
</ul>
<h4 id="batch-normalization">Batch Normalization</h4>
<p>Batch Normalization (BN) is a technique that stabilizes and speeds up
the learning process by normalizing the inputs of each layer. By
adjusting and scaling the activations, it ensures that the distribution
of the inputs to a layer doesn't change too much, which can prevent
issues like the vanishing or exploding gradient problems.</p>
<p><strong>Performance Impact</strong>:</p>
<ul>
<li><strong>Training Speed</strong>: Typically makes the network faster
to train. It allows for the use of higher learning rates and reduces the
sensitivity to initialization.</li>
<li><strong>Convergence</strong>: Networks often converge quicker with
batch normalization.</li>
<li><strong>Model Performance</strong>: Can improve model performance,
as it has a slight regularization effect.</li>
<li><strong>Inference Time</strong>: Can slightly increase inference
time as there are extra computations for normalization; however, this
can be mitigated by folding BN into the preceding layers during
optimization.</li>
</ul>
<h4 id="combining-dropout-and-batch-normalization">Combining Dropout and
Batch Normalization</h4>
<p>While both methods have their pros and cons, there's often a benefit
in combining them, but the order of operations matters. Generally, BN is
applied right after convolutional layers (and before non-linearities),
while dropout is placed after the activation functions. There has been
some debate and research into the proper ordering of these layers, and
it may depend on the specific architecture and task.</p>
<h4 id="experimental-validation">Experimental Validation</h4>
<p>The ultimate performance metrics for these methods come from
empirical experimentation:</p>
<ul>
<li><strong>A/B Testing</strong>: Run experiments with different
combinations of dropout and batch normalization to see which yields the
best results on your validation data.</li>
<li><strong>Monitor Training/Validation Loss</strong>: Track the
training and validation loss over epochs; batch normalization typically
smooths out training curves, whereas dropout can make them noisier but
usually increases validation performance.</li>
<li><strong>Early Stopping</strong>: Both techniques can change the
ideal stopping point for training, so using an early-stopping criterion
based on validation loss can be beneficial.</li>
</ul>
<p>In summary, dropout is primarily a regularization method to combat
overfitting, and batch normalization helps in normalizing the input
distribution to each layer, thus speeding up the training and
potentially allowing for better model performance. The effects on
performance should be considered in the context of the specific
architecture and problem being addressed.</p>
</section>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T07:18:22.454837Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T07:18:22.453899Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T07:18:22.460559Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T07:18:22.459618Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T07:18:22.454805Z&quot;}"
id="0miHtwa9UvED">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>test_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset <span class="op">=</span> random_split(dataset, [train_size, test_size])</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> DataLoader(test_dataset, batch_size<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T07:18:56.797430Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T07:18:56.796660Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T07:19:01.197479Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T07:19:01.196461Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T07:18:56.797380Z&quot;}"
id="LX0Uuodp9fV2">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelBrain(nn.Module):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ModelBrain, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_layer <span class="op">=</span> nn.Sequential(</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(num_channels, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">16</span>),</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">32</span>),</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">64</span>),</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._to_linear <span class="op">=</span> <span class="dv">16</span> <span class="op">*</span> <span class="dv">124</span> <span class="op">*</span> <span class="dv">124</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_layer <span class="op">=</span> nn.Sequential(</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(),</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>._to_linear, hidden_units),</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_units, hidden_units <span class="op">//</span> <span class="dv">4</span>),</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>            nn.LeakyReLU(),</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_units <span class="op">//</span> <span class="dv">4</span>, output_shape)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv_layer(x)</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (<span class="va">self</span>._to_linear <span class="op">==</span> <span class="dv">16</span> <span class="op">*</span> <span class="dv">124</span> <span class="op">*</span> <span class="dv">124</span>):</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._to_linear <span class="op">=</span> x[<span class="dv">0</span>].nelement()</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>._to_linear)</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.fc_layer(x)</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>num_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>hidden_units <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>output_shape <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ModelBrain(num_channels<span class="op">=</span>num_channels, hidden_units<span class="op">=</span>hidden_units, output_shape<span class="op">=</span>output_shape)</span></code></pre></div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T07:19:01.199361Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T07:19:01.199045Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T07:19:01.880849Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T07:19:01.880038Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T07:19:01.199334Z&quot;}"
id="1l8X31E2UvED">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;referenced_widgets&quot;:[&quot;7240ba19d41f42d1a966f766ca4dafeb&quot;]}"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T07:19:05.647087Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T07:19:05.646308Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T07:30:09.249356Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T07:30:09.248274Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T07:19:05.647050Z&quot;}"
id="sqOnedDdUvED" data-outputId="d25346b8-8667-4c37-bc66-c06a0edd803e">
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Lists to keep track of progress</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>epoch_count <span class="op">=</span> []</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>train_losses <span class="op">=</span> []</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>test_losses <span class="op">=</span> []</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>train_accuracies <span class="op">=</span> []</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>test_accuracies <span class="op">=</span> []</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    correct_train <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    correct_test <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    total_train <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    total_test <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Epoch </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(epoch<span class="op">+</span><span class="dv">1</span>, epochs))</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;-&quot;</span><span class="op">*</span><span class="dv">10</span>)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    model.train()  <span class="co"># Set model to training mode</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels, _ <span class="kw">in</span> train_loader:</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.to(device)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> labels.to(device)</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()  <span class="co"># Clear gradients</span></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(outputs, labels)</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>        total_train <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>        correct_train <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set model to evaluation mode</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels, _ <span class="kw">in</span> test_loader:</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> images.to(device)</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> labels.to(device)</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(images)</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(outputs, labels)</span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss.item() <span class="op">*</span> images.size(<span class="dv">0</span>)</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>            total_test <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>            correct_test <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train_loss <span class="op">/</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> test_loss <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>    train_accuracy <span class="op">=</span> correct_train <span class="op">/</span> total_train <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>    test_accuracy <span class="op">=</span> correct_test <span class="op">/</span> total_test <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Train Loss: </span><span class="sc">{:.4f}</span><span class="st">, Train Accuracy: </span><span class="sc">{:.2f}</span><span class="st">%&quot;</span>.<span class="bu">format</span>(train_loss, train_accuracy))</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Test Loss: </span><span class="sc">{:.4f}</span><span class="st">, Test Accuracy: </span><span class="sc">{:.2f}</span><span class="st">%&quot;</span>.<span class="bu">format</span>(test_loss, test_accuracy))</span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>    epoch_count.append(epoch<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>    train_losses.append(train_loss)</span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>    test_losses.append(test_loss)</span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a>    train_accuracies.append(train_accuracy)</span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a>    test_accuracies.append(test_accuracy)</span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-77"><a href="#cb32-77" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb32-78"><a href="#cb32-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, train_losses, label<span class="op">=</span><span class="st">&#39;Train Loss&#39;</span>)</span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, test_losses, label<span class="op">=</span><span class="st">&#39;Test Loss&#39;</span>)</span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Loss over Epochs&#39;</span>)</span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb32-86"><a href="#cb32-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-87"><a href="#cb32-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-88"><a href="#cb32-88" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb32-89"><a href="#cb32-89" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, train_accuracies, label<span class="op">=</span><span class="st">&#39;Train Accuracy&#39;</span>)</span>
<span id="cb32-90"><a href="#cb32-90" aria-hidden="true" tabindex="-1"></a>plt.plot(epoch_count, test_accuracies, label<span class="op">=</span><span class="st">&#39;Test Accuracy&#39;</span>)</span>
<span id="cb32-91"><a href="#cb32-91" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb32-92"><a href="#cb32-92" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Accuracy (%)&#39;</span>)</span>
<span id="cb32-93"><a href="#cb32-93" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Accuracy over Epochs&#39;</span>)</span>
<span id="cb32-94"><a href="#cb32-94" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb32-95"><a href="#cb32-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-96"><a href="#cb32-96" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb33"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;7240ba19d41f42d1a966f766ca4dafeb&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>Epoch 1/9
----------
Train Loss: 4.1559, Train Accuracy: 48.57%
Test Loss: 1.0638, Test Accuracy: 56.65%
Epoch 2/9
----------
Train Loss: 1.3740, Train Accuracy: 54.01%
Test Loss: 0.8248, Test Accuracy: 64.26%
Epoch 3/9
----------
Train Loss: 1.2317, Train Accuracy: 62.31%
Test Loss: 1.8175, Test Accuracy: 60.84%
Epoch 4/9
----------
Train Loss: 0.8846, Train Accuracy: 71.28%
Test Loss: 1.6185, Test Accuracy: 54.75%
Epoch 5/9
----------
Train Loss: 1.1297, Train Accuracy: 65.94%
Test Loss: 1.0152, Test Accuracy: 55.51%
Epoch 6/9
----------
Train Loss: 0.7998, Train Accuracy: 75.29%
Test Loss: 0.7170, Test Accuracy: 70.34%
Epoch 7/9
----------
Train Loss: 0.6232, Train Accuracy: 79.39%
Test Loss: 0.5853, Test Accuracy: 77.19%
Epoch 8/9
----------
Train Loss: 0.4813, Train Accuracy: 85.31%
Test Loss: 0.5617, Test Accuracy: 79.47%
Epoch 9/9
----------
Train Loss: 0.3414, Train Accuracy: 89.98%
Test Loss: 0.5957, Test Accuracy: 82.13%
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/c9e055f7d87ce0ba69896372e8c8ea753dec5634.png" /></p>
</div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T07:30:09.252196Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T07:30:09.251578Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T07:30:16.187498Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T07:30:16.186333Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T07:30:09.252156Z&quot;}"
id="bVk8XyjjUvEE" data-outputId="69bee64f-2eae-4566-afd2-d197d752a71c">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> eval_model(model ,test_loader )</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/4bbfa62a8b7fc6c8dfc0b3070b80a0e3b612d1f2.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Test Loss is:0.5957
Accuracy of the model: 82.13%
</code></pre>
</div>
</div>
<div class="cell code"
data-execution="{&quot;iopub.execute_input&quot;:&quot;2024-01-07T07:30:16.189449Z&quot;,&quot;iopub.status.busy&quot;:&quot;2024-01-07T07:30:16.189056Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-01-07T07:30:16.457242Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-01-07T07:30:16.456253Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-01-07T07:30:16.189419Z&quot;}"
id="C4Oe-NobUvEE" data-outputId="1623647a-7d5e-4914-96f0-141d49b9ef4f">
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>scoring(cm)</span></code></pre></div>
<div class="output display_data">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_70d2ba020a79466692e66b232fdaa8a7/d7e2740045d219543ae0ae472cdbea1337b78f10.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Precision: 0.90
Recall: 0.98
F1 Score: 0.94

micro average: 1.0266159695817492
weighted_average: 1.1555224575054388
macro average: 1.1555224575054388
accuract: 0.8212927756653993
</code></pre>
</div>
</div>
<div class="cell markdown" id="UM97sdcDUvEF">
<p>As it can be seen, when we use regularization methods, the model is
overfit much later. These methods help to reduce the very large
coefficients in the model and the extremely high complexity is less
likely. After doing this method, you will get the following results,
which are a little better than the results without regularization</p>
</div>
<div class="cell markdown" id="C9gnbZOsUvEV">
<p>The important problem in these methods is that the model probably
converges later</p>
</div>
</body>
</html>
