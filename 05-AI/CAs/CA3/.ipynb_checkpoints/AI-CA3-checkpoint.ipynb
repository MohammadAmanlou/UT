{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, num_hidden_states):\n",
    "        self.num_hidden_states = num_hidden_states\n",
    "        self.rand_state = np.random.RandomState(1)\n",
    "\n",
    "        self.initial_prob = self._normalize(self.rand_state.rand(self.num_hidden_states, 1))\n",
    "        self.transition_matrix = self._stochasticize(self.rand_state.rand(self.num_hidden_states, self.num_hidden_states))\n",
    "\n",
    "        self.mean = None\n",
    "        self.covariances = None\n",
    "        self.num_dimensions = None\n",
    "\n",
    "    def _forward(self, observation_matrix):\n",
    "        log_likelihood = 0.\n",
    "        T = observation_matrix.shape[1]\n",
    "        alpha = np.zeros(observation_matrix.shape)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                alpha[:, t] = self.initial_prob * self._state_likelihood(observation_matrix[:, t])\n",
    "            else:\n",
    "                alpha[:, t] = np.dot(self.transition_matrix.T, alpha[:, t-1]) * self._state_likelihood(observation_matrix[:, t])\n",
    "\n",
    "            alpha_sum = np.sum(alpha[:, t])\n",
    "            alpha[:, t] /= alpha_sum\n",
    "            log_likelihood += np.log(alpha_sum)\n",
    "\n",
    "        return log_likelihood, alpha\n",
    "\n",
    "    def _backward(self, observation_matrix):\n",
    "        T = observation_matrix.shape[1]\n",
    "        beta = np.zeros(observation_matrix.shape)\n",
    "\n",
    "        beta[:, -1] = np.ones(observation_matrix.shape[0])\n",
    "\n",
    "        for t in range(T - 1)[::-1]:\n",
    "            beta[:, t] = np.dot(self.transition_matrix, (self._state_likelihood(observation_matrix[:, t+1]) * beta[:, t+1]))\n",
    "            beta[:, t] /= np.sum(beta[:, t])\n",
    "\n",
    "        return beta\n",
    "\n",
    "    def _state_likelihood(self, obs):\n",
    "        obs = np.atleast_2d(obs)\n",
    "        B = np.zeros((self.num_hidden_states, obs.shape[1]))\n",
    "\n",
    "        for s in range(self.num_hidden_states):\n",
    "            np.random.seed(self.rand_state.randint(1))\n",
    "            B[s, :] = multivariate_normal.pdf(obs.T, mean=self.mean[:, s], cov=self.covariances[:, :, s])\n",
    "\n",
    "        return B\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        return (x + (x == 0)) / np.sum(x)\n",
    "\n",
    "    def _stochasticize(self, x):\n",
    "        return (x + (x == 0)) / np.sum(x, axis=1)\n",
    "\n",
    "    def _em_init(self, obs):\n",
    "        if self.num_dimensions is None:\n",
    "            self.num_dimensions = obs.shape[0]\n",
    "        if self.mean is None:\n",
    "            subset = self.rand_state.choice(np.arange(self.num_dimensions), size=self.num_hidden_states, replace=False)\n",
    "            self.mean = obs[:, subset]\n",
    "        if self.covariances is None:\n",
    "            self.covariances = np.zeros((self.num_dimensions, self.num_dimensions, self.num_hidden_states))\n",
    "            self.covariances += np.diag(np.diag(np.cov(obs)))[:, :, None]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _em_step(self, obs):\n",
    "        obs = np.atleast_2d(obs)\n",
    "        T = obs.shape[1]\n",
    "\n",
    "        B = self._state_likelihood(obs)\n",
    "\n",
    "        log_likelihood, alpha = self._forward(B)\n",
    "        beta = self._backward(B)\n",
    "\n",
    "        xi_sum = np.zeros((self.num_hidden_states, self.num_hidden_states))\n",
    "        gamma = np.zeros((self.num_hidden_states, T))\n",
    "\n",
    "        for t in range(T - 1):\n",
    "            partial_sum = np.dot(alpha[:, t], beta[:, t]) * self.transition_matrix\n",
    "            xi_sum += self._normalize(partial_sum)\n",
    "            partial_g = alpha[:, t] * beta[:, t]\n",
    "            gamma[:, t] = self._normalize(partial_g)\n",
    "        partial_g = alpha[:, -1] * beta[:, -1]\n",
    "        gamma[:, -1] = self._normalize(partial_g)\n",
    "\n",
    "        expected_prior = gamma[:, 0]\n",
    "        expected_transition = self._stochasticize(xi_sum)\n",
    "\n",
    "        expected_covariances = np.zeros((self.num_dimensions, self.num_dimensions, self.num_hidden_states))\n",
    "        expected_covariances += .01 * np.eye(self.num_dimensions)[:, :, None]\n",
    "\n",
    "        gamma_state_sum = np.sum(gamma, axis=1)\n",
    "        gamma_state_sum = gamma_state_sum + (gamma_state_sum == 0)\n",
    "\n",
    "        expected_mean = np.zeros((self.num_dimensions, self.num_hidden_states))\n",
    "        for s in range(self.num_hidden_states):\n",
    "            gamma_obs = obs * gamma[s, :]\n",
    "            expected_mean[:, s] = np.sum(gamma_obs, axis=1) / gamma_state_sum[s]\n",
    "\n",
    "        self.initial_prob = expected_prior\n",
    "        self.mean = expected_mean\n",
    "        self.covariances = expected_covariances\n",
    "        self.transition_matrix = expected_transition\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def train(self, obs, num_iterations=1):\n",
    "        for i in range(num_iterations):\n",
    "            self._em_init(obs)\n",
    "            self._em_step(obs)\n",
    "        return self\n",
    "\n",
    "    def score(self, obs):\n",
    "        B = self._state_likelihood(obs)\n",
    "        log_likelihood, _ = self._forward(B)\n",
    "        return log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa as lb\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# show sample mfcc\n",
    "new_rate = 232 # what this do?\n",
    "(rate,sig) = wav.read(\"blues/blues.00000.wav\")\n",
    "#number_of_samples = round(len(sig) * float(new_rate) / rate)\n",
    "#sig = sps.resample(sig,number_of_samples)\n",
    "mfcc_data = mfcc(sig,rate, nfft=551)\n",
    "fig, ax = plt.subplots()\n",
    "mfcc_data = np.swapaxes(mfcc_data, 0, 1)\n",
    "cax = ax.imshow(mfcc_data, interpolation='nearest', cmap=cm.coolwarm, origin='lower')\n",
    "ax.set_title('MFCC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# In this section, iterate through the audio files, extract\n",
    "# features, and use the trained HMMs to predict the genre\n",
    "# for each audio file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
